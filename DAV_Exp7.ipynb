{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPs5d1PibMjrySzkzAWurjG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedantrk/DAV-SEM6/blob/main/DAV_Exp7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment - 7:** Perform the steps involved in Text Analytics in Python & R\n",
        "\n",
        "**Task to be performed :**\n",
        "\n",
        "Explore Top-5 Text Analytics Libraries in Python (w.r.t Features & Applications)\n",
        "\n",
        "Explore Top-5 Text Analytics Libraries in R (w.r.t Features & Applications)\n",
        "\n",
        "Perform the following experiments using Python & R\n",
        "\n",
        "Tokenization (Sentence & Word)\n",
        "\n",
        "Frequency Distribution\n",
        "\n",
        "Remove stopwords & punctuations\n",
        "\n",
        "Lexicon Normalization (Stemming, Lemmatization)\n",
        "\n",
        "Part of Speech tagging\n",
        "\n",
        "Named Entity Recognization\n",
        "\n",
        "Scrape data from a website\n",
        "\n",
        "Prepare a document with the Aim, Tasks performed, Program, Output, and Conclusion."
      ],
      "metadata": {
        "id": "ol4ZLK0xv7c8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Program:**"
      ],
      "metadata": {
        "id": "aB6b9PF6xmEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "j5GkSMlCxo80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "496005eb-b55c-4f52-d72f-aa6c84cc0f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''If you're visiting this page, you're likely here because you're searching for a random sentence.'''"
      ],
      "metadata": {
        "id": "7TEiGq6HFm5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing into sentences\n",
        "sentences = nltk.tokenize.sent_tokenize(text)\n",
        "\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVMHRI8hF9lk",
        "outputId": "724be60d-95aa-4b80-cc42-4271da294cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"If you're visiting this page, you're likely here because you're searching for a random sentence.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing into words\n",
        "words = nltk.tokenize.word_tokenize(text)\n",
        "\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtHd84W3F9os",
        "outputId": "59e9df14-6be8-4c93-f4f7-b3877ec23742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['If', 'you', \"'re\", 'visiting', 'this', 'page', ',', 'you', \"'re\", 'likely', 'here', 'because', 'you', \"'re\", 'searching', 'for', 'a', 'random', 'sentence', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequency Distribution of words\n",
        "freq_dis = nltk.FreqDist(words)\n",
        "\n",
        "print(freq_dis.most_common(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpYp0NtSF9zU",
        "outputId": "880293f4-9b4a-4fb7-bd4b-61c912d16705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('you', 3), (\"'re\", 3), ('If', 1), ('visiting', 1), ('this', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdIn9iWpGpBd",
        "outputId": "37ae0770-db86-4db9-de24-dd248cef1edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'very', 'with', 'from', 'between', 'weren', 'where', \"couldn't\", 'will', 'couldn', 'now', 'is', 'once', 'isn', 'down', 'yourselves', 'then', 'only', 'wouldn', 'them', 'through', 'further', 'more', 'or', 'who', 'at', 'me', 'same', 'this', 'so', 'myself', 'needn', 'nor', \"won't\", 'about', \"shouldn't\", 'ain', 'we', 'won', 'what', 'am', 'both', 'some', 'doing', 'again', \"hadn't\", 'i', 'but', 'all', \"aren't\", 'him', \"weren't\", 're', 'by', 'an', 'my', 'such', 'your', 'its', 'aren', \"mightn't\", 'her', 'does', 'of', 'hadn', 'himself', 'it', 'to', 'our', 'while', 'other', 'as', 'own', \"that'll\", 'were', 'and', 'which', 'any', \"you'll\", 'she', 'you', 's', 'are', 've', 'that', 'in', 'for', \"you're\", 'can', 'no', 'their', 'should', 'didn', 'the', 'whom', 'themselves', 'why', 'y', \"haven't\", 'over', 'theirs', 'm', 'herself', 'mustn', \"needn't\", 'll', 'below', 't', 'shouldn', \"you've\", 'before', 'few', 'don', 'did', 'ourselves', 'most', 'when', \"wouldn't\", 'just', 'after', 'have', 'those', 'into', 'has', 'hers', 'until', 'his', \"you'd\", 'he', \"hasn't\", 'ma', \"wasn't\", 'they', 'be', \"isn't\", 'mightn', 'wasn', 'on', \"mustn't\", \"didn't\", 'was', 'doesn', 'if', 'been', 'up', 'haven', 'hasn', 'being', 'how', 'these', 'against', 'above', \"shan't\", 'yours', 'because', 'each', \"it's\", 'had', 'shan', \"should've\", 'off', 'itself', 'having', \"doesn't\", 'out', \"don't\", 'do', 'o', 'there', 'a', 'under', 'here', 'too', 'during', 'yourself', \"she's\", 'not', 'than', 'd', 'ours'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Filtering Stopwords\n",
        "\n",
        "filtered_words = []\n",
        "\n",
        "for i in words:\n",
        "  if i not in stop_words:\n",
        "    filtered_words.append(i)\n",
        "\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ViHsiAWIFv0",
        "outputId": "6c54a948-e6fd-45b5-de7c-ff77ded89215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['If', \"'re\", 'visiting', 'page', ',', \"'re\", 'likely', \"'re\", 'searching', 'random', 'sentence', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter out Punctuation\n",
        "\n",
        "filtered_words2 = []\n",
        "\n",
        "import string\n",
        "\n",
        "punctuations=list(string.punctuation)\n",
        "\n",
        "for i in filtered_words:\n",
        "    if i not in punctuations:\n",
        "        filtered_words2.append(i)\n",
        "\n",
        "print(filtered_words2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_fPg-KxGpFT",
        "outputId": "dc0492e7-7bdf-478c-ab94-8d4de053f2b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['If', \"'re\", 'visiting', 'page', \"'re\", 'likely', \"'re\", 'searching', 'random', 'sentence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "stemmed_words = []\n",
        "\n",
        "for i in filtered_words2:\n",
        "  stemmed_words.append(stemmer.stem(i))\n",
        "\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTj25qjFGpJN",
        "outputId": "7d7afad8-26aa-4559-fef7-4193ccb0c084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['if', \"'re\", 'visit', 'page', \"'re\", 'like', \"'re\", 'search', 'random', 'sentenc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lem = WordNetLemmatizer()\n",
        "word = \"flying\"\n",
        "\n",
        "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjE8DgNAIV03",
        "outputId": "60d68578-e310-455f-d7db-bf00e73b46f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Word: fly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#POS Tagging\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sent = \"Albert Einstein was born in Ulm, Germany in 1879.\"\n",
        "\n",
        "tokens=word_tokenize(sent)\n",
        "pos_=pos_tag(tokens)\n",
        "\n",
        "print(\"PoS tags:\",pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooWsD2etJRDm",
        "outputId": "2ac529f4-ac84-40b1-db2b-41ec97f9cde5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PoS tags: [('Albert', 'NNP'), ('Einstein', 'NNP'), ('was', 'VBD'), ('born', 'VBN'), ('in', 'IN'), ('Ulm', 'NNP'), (',', ','), ('Germany', 'NNP'), ('in', 'IN'), ('1879', 'CD'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Name Entity Recognition\n",
        "\n",
        "from nltk import ne_chunk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "sent=\"New York City on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.\"\n",
        "\n",
        "for chunk in ne_chunk(nltk.pos_tag(word_tokenize(sent))):\n",
        "        if hasattr(chunk, 'label'):\n",
        "            print(chunk.label(), ' '.join(c[0] for c in chunk))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9zvTxBCJRGl",
        "outputId": "d99304b6-d58a-47e8-f8da-4c47f9a12ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPE New York City\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib1l80dWJRJd",
        "outputId": "c9a87189-5687-4dc8-fcec-7e344df7df84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from requests import HTTPError"
      ],
      "metadata": {
        "id": "Q4gV3g7ZJRLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://github.com/LifnaJos/ADL601-Data-Analytics-and-Visualization-Lab/blob/main/Experiments/Experiment_7.md'\n",
        "\n",
        "try:\n",
        "  response = requests.get(url)\n",
        "except HTTPError as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "48zgaewqJeEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(response.text, 'html.parser')"
      ],
      "metadata": {
        "id": "mhkHKp-WJeHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links = soup.find_all('a')\n",
        "\n",
        "for link in links:\n",
        "    print(link.get('href'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFzdwfd0JeKr",
        "outputId": "d184d9c1-e937-4f5c-ba19-b42ad3a74234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\\"#experiment---7-perform-the-steps-involved-in-text-analytics-in-python--r\\\"\n",
            "\\\"#lab-outcomes-lo\\\"\n",
            "\\\"#task-to-be-performed-\\\"\n",
            "\\\"#tools--libraries-to-be-explored\\\"\n",
            "\\\"#theory-to-be-written\\\"\n",
            "\\\"#outcome-\\\"\n",
            "\\\"#online-resources\\\"\n",
            "\\\"https://github.com/LifnaJos/ADC601-Data-Analytics-Visualization/blob/DAV_Colab_Notebooks/Data_Preprocessing_techniques.ipynb\\\"\n",
            "\\\"https://guides.library.upenn.edu/penntdm/python\\\"\n",
            "\\\"https://machinelearninggeek.com/text-analytics-for-beginners-using-python-nltk/\\\"\n",
            "\\\"https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\\\"\n",
            "\\\"https://www.kdnuggets.com/2020/05/text-mining-python-steps-examples.html\\\"\n",
            "\\\"https://www.youtube.com/watch?v=bZoC-UW50sI&list=PLH6mU1kedUy-xjgiuvqMkVn8npK0TGAv5\\\"\n",
            "\\\"https://www.analyticsvidhya.com/blog/2022/07/sentiment-analysis-using-python/\\\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**R**"
      ],
      "metadata": {
        "id": "ylT32bHWKTmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"tokenizers\")\n",
        "install.packages(\"tm\")\n",
        "install.packages(\"udpipe\")\n",
        "install.packages(\"spacyr\")\n",
        "install.packages(\"rvest\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_lWHZ2pJeNk",
        "outputId": "c605285f-ee4f-4ef9-b430-bd585924b272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘Rcpp’, ‘SnowballC’\n",
            "\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘NLP’, ‘slam’, ‘BH’\n",
            "\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘RcppTOML’, ‘here’, ‘png’, ‘reticulate’\n",
            "\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "library(tokenizers)\n",
        "library(tm)\n",
        "library(udpipe)\n",
        "library(spacyr)\n",
        "library(rvest)"
      ],
      "metadata": {
        "id": "vwjMiB_GKEk1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63eafc93-971b-4b4e-9584-53657381f45b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading required package: NLP\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "text <- 'Suave, charming and volatile, Reggie Kray and his unstable twin brother Ronnie start to leave their mark on the London underworld in the 1960s. Using violence to get what they want, the siblings orchestrate robberies and murders while running nightclubs and protection rackets. With police Detective Leonard \"Nipper\" Read hot on their heels, the brothers continue their rapid rise to power and achieve tabloid notoriety.'\n",
        "\n",
        "word_tokens <- tokenize_words(text)\n",
        "sent_tokens <- tokenize_sentences(text)\n",
        "\n",
        "print(\"Sentence Tokens:\")\n",
        "print(sent_tokens)\n",
        "print(\"Word Tokens:\")\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "id": "ukA4k9UdKEn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2adea512-c0f2-4545-ad61-d40348fc09f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Sentence Tokens:\"\n",
            "[[1]]\n",
            "[1] \"Suave, charming and volatile, Reggie Kray and his unstable twin brother Ronnie start to leave their mark on the London underworld in the 1960s.\" \n",
            "[2] \"Using violence to get what they want, the siblings orchestrate robberies and murders while running nightclubs and protection rackets.\"           \n",
            "[3] \"With police Detective Leonard \\\"Nipper\\\" Read hot on their heels, the brothers continue their rapid rise to power and achieve tabloid notoriety.\"\n",
            "\n",
            "[1] \"Word Tokens:\"\n",
            "[[1]]\n",
            " [1] \"suave\"       \"charming\"    \"and\"         \"volatile\"    \"reggie\"     \n",
            " [6] \"kray\"        \"and\"         \"his\"         \"unstable\"    \"twin\"       \n",
            "[11] \"brother\"     \"ronnie\"      \"start\"       \"to\"          \"leave\"      \n",
            "[16] \"their\"       \"mark\"        \"on\"          \"the\"         \"london\"     \n",
            "[21] \"underworld\"  \"in\"          \"the\"         \"1960s\"       \"using\"      \n",
            "[26] \"violence\"    \"to\"          \"get\"         \"what\"        \"they\"       \n",
            "[31] \"want\"        \"the\"         \"siblings\"    \"orchestrate\" \"robberies\"  \n",
            "[36] \"and\"         \"murders\"     \"while\"       \"running\"     \"nightclubs\" \n",
            "[41] \"and\"         \"protection\"  \"rackets\"     \"with\"        \"police\"     \n",
            "[46] \"detective\"   \"leonard\"     \"nipper\"      \"read\"        \"hot\"        \n",
            "[51] \"on\"          \"their\"       \"heels\"       \"the\"         \"brothers\"   \n",
            "[56] \"continue\"    \"their\"       \"rapid\"       \"rise\"        \"to\"         \n",
            "[61] \"power\"       \"and\"         \"achieve\"     \"tabloid\"     \"notoriety\"  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Frequency Distribution\n",
        "word_freq <- table(word_tokens)\n",
        "\n",
        "print(\"Word Frequency Distribution:\")\n",
        "print(word_freq)"
      ],
      "metadata": {
        "id": "mzs1YiowKEqt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56496d6-b109-4140-f658-c05843d3b6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Word Frequency Distribution:\"\n",
            "word_tokens\n",
            "      1960s     achieve         and     brother    brothers    charming \n",
            "          1           1           5           1           1           1 \n",
            "   continue   detective         get       heels         his         hot \n",
            "          1           1           1           1           1           1 \n",
            "         in        kray       leave     leonard      london        mark \n",
            "          1           1           1           1           1           1 \n",
            "    murders  nightclubs      nipper   notoriety          on orchestrate \n",
            "          1           1           1           1           2           1 \n",
            "     police       power  protection     rackets       rapid        read \n",
            "          1           1           1           1           1           1 \n",
            "     reggie        rise   robberies      ronnie     running    siblings \n",
            "          1           1           1           1           1           1 \n",
            "      start       suave     tabloid         the       their        they \n",
            "          1           1           1           4           3           1 \n",
            "         to        twin  underworld    unstable       using    violence \n",
            "          3           1           1           1           1           1 \n",
            "   volatile        want        what       while        with \n",
            "          1           1           1           1           1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_freq <- sort(word_freq, decreasing = TRUE)\n",
        "\n",
        "print(\"Top 5 most used words:\")\n",
        "print(sorted_freq[1:5])"
      ],
      "metadata": {
        "id": "SgLBueruKEts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0c0b790-129d-4d76-b84e-420929f171d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Top 5 most used words:\"\n",
            "word_tokens\n",
            "  and   the their    to    on \n",
            "    5     4     3     3     2 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Punctuation and Stop word removal\n",
        "corpus <- Corpus(VectorSource(text))\n",
        "\n",
        "corpus <- tm_map(corpus, content_transformer(tolower))\n",
        "corpus <- tm_map(corpus, removePunctuation)\n",
        "corpus <- tm_map(corpus, removeWords, stopwords(\"english\"))\n",
        "\n",
        "cleaned_text <- sapply(corpus, as.character)\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "id": "DAnJQ-LRKthM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c5c11be-0ba7-4253-846f-9992a6f34e2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning message in tm_map.SimpleCorpus(corpus, content_transformer(tolower)):\n",
            "“transformation drops documents”\n",
            "Warning message in tm_map.SimpleCorpus(corpus, removePunctuation):\n",
            "“transformation drops documents”\n",
            "Warning message in tm_map.SimpleCorpus(corpus, removeWords, stopwords(\"english\")):\n",
            "“transformation drops documents”\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"suave charming  volatile reggie kray   unstable twin brother ronnie start  leave  mark   london underworld   1960s using violence  get   want  siblings orchestrate robberies  murders  running nightclubs  protection rackets  police detective leonard nipper read hot   heels  brothers continue  rapid rise  power  achieve tabloid notoriety\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lexicon Normalization\n",
        "ud_model <- udpipe_download_model(language = \"english\")\n",
        "ud_model <- udpipe_load_model(ud_model$file_model)"
      ],
      "metadata": {
        "id": "g8Uix3QeKtkT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c66fd2d-5e4c-471f-8dcd-e30620e7458e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading udpipe model from https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.5/master/inst/udpipe-ud-2.5-191206/english-ewt-ud-2.5-191206.udpipe to /content/english-ewt-ud-2.5-191206.udpipe\n",
            "\n",
            " - This model has been trained on version 2.5 of data from https://universaldependencies.org\n",
            "\n",
            " - The model is distributed under the CC-BY-SA-NC license: https://creativecommons.org/licenses/by-nc-sa/4.0\n",
            "\n",
            " - Visit https://github.com/jwijffels/udpipe.models.ud.2.5 for model license details.\n",
            "\n",
            " - For a list of all models and their licenses (most models you can download with this package have either a CC-BY-SA or a CC-BY-SA-NC license) read the documentation at ?udpipe_download_model. For building your own models: visit the documentation by typing vignette('udpipe-train', package = 'udpipe')\n",
            "\n",
            "Downloading finished, model stored at '/content/english-ewt-ud-2.5-191206.udpipe'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens <- udpipe_annotate(ud_model, x = text)\n",
        "\n",
        "lemmas <- as.data.frame(tokens)$lemma\n",
        "cat(\"Lemmas:\", lemmas, \"\\n\")\n",
        "\n",
        "stems <- as.data.frame(tokens)$token\n",
        "cat(\"Stems:\", stems, \"\\n\")"
      ],
      "metadata": {
        "id": "KprnY4JTKtrt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3b13fef-8b09-47bf-ca3c-75e607900b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmas: Suave , charming and volatile , Reggie Kray and he unstable twin brother Ronnie start to leave they mark on the London underworld in the 1960 . use violence to get what they want , the sibling orchestrate robbery and murder while run nightclub and protection racket . with police detective Leonard \" Nipper \" read hot on they heel , the brother continue they rapid rise to power and achieve tabloid notoriety . \n",
            "Stems: Suave , charming and volatile , Reggie Kray and his unstable twin brother Ronnie start to leave their mark on the London underworld in the 1960s . Using violence to get what they want , the siblings orchestrate robberies and murders while running nightclubs and protection rackets . With police Detective Leonard \" Nipper \" Read hot on their heels , the brothers continue their rapid rise to power and achieve tabloid notoriety . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#POS tagging\n",
        "pos_tags <- as.data.frame(tokens)$upos\n",
        "cat(\"POS tags:\", pos_tags, \"\\n\")"
      ],
      "metadata": {
        "id": "0sT8z1HQK1nM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e2b5d50-933e-47d9-c57f-0d488a55e273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tags: PROPN PUNCT NOUN CCONJ NOUN PUNCT PROPN PROPN CCONJ PRON ADJ NOUN NOUN PROPN VERB PART VERB PRON NOUN ADP DET PROPN NOUN ADP DET NOUN PUNCT VERB NOUN PART VERB PRON PRON VERB PUNCT DET NOUN ADJ NOUN CCONJ NOUN SCONJ VERB NOUN CCONJ NOUN NOUN PUNCT ADP NOUN PROPN PROPN PUNCT PROPN PUNCT VERB ADJ ADP PRON NOUN PUNCT DET NOUN VERB PRON ADJ NOUN ADP NOUN CCONJ NOUN NOUN NOUN PUNCT \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Web Scraping\n",
        "url <- \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
        "webpage <- read_html(url)\n",
        "\n",
        "article_titles <- webpage %>%\n",
        "  html_nodes(\".text\") %>%\n",
        "  html_text()\n",
        "\n",
        "print(article_titles)"
      ],
      "metadata": {
        "id": "cA40Tx7vK1ps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ae39acc-d5fc-4599-bdbb-b781e95fd828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [1] \"\\\"Web scraping\\\"\"                                                                                                                   \n",
            " [2] \"news\"                                                                                                                               \n",
            " [3] \"newspapers\"                                                                                                                         \n",
            " [4] \"books\"                                                                                                                              \n",
            " [5] \"scholar\"                                                                                                                            \n",
            " [6] \"JSTOR\"                                                                                                                              \n",
            " [7] \"improve this section\"                                                                                                               \n",
            " [8] \"\\\"SASSCAL WebSAPI: A Web Scraping Application Programming Interface to Support Access to SASSCAL's Weather Data\\\"\"                  \n",
            " [9] \"10.5334/dsj-2021-024\"                                                                                                               \n",
            "[10] \"1683-1470\"                                                                                                                          \n",
            "[11] \"237719804\"                                                                                                                          \n",
            "[12] \"\\\"Search Engine History.com\\\"\"                                                                                                      \n",
            "[13] \"\\\"Joint optimization of wrapper generation and template detection\\\"\"                                                                \n",
            "[14] \"10.1145/1281192.1281287\"                                                                                                            \n",
            "[15] \"833565\"                                                                                                                             \n",
            "[16] \"the original\"                                                                                                                       \n",
            "[17] \"Semantic annotation based web scraping\"                                                                                             \n",
            "[18] \"\\\"Diffbot Is Using Computer Vision to Reinvent the Semantic Web\\\"\"                                                                  \n",
            "[19] \"\\\"TUTORIAL: AI research without coding: The art of fighting without fighting: Data science for qualitative researchers\\\"\"           \n",
            "[20] \"10.1016/j.jbusres.2020.06.012\"                                                                                                      \n",
            "[21] \"0148-2963\"                                                                                                                          \n",
            "[22] \"\\\"FAQ about linking – Are website terms of use binding contracts?\\\"\"                                                                \n",
            "[23] \"the original\"                                                                                                                       \n",
            "[24] \"\\\"Symbiotic Relationships: Pragmatic Acceptance of Data Scraping\\\"\"                                                                 \n",
            "[25] \"10.15779/Z38B39B\"                                                                                                                   \n",
            "[26] \"1086-3818\"                                                                                                                          \n",
            "[27] \"\\\"Internet Law, Ch. 06: Trespass to Chattels\\\"\"                                                                                     \n",
            "[28] \"\\\"What are the \\\"trespass to chattels\\\" claims some companies or website owners have brought?\\\"\"                                    \n",
            "[29] \"the original\"                                                                                                                       \n",
            "[30] \"\\\"Ticketmaster Corp. v. Tickets.com, Inc\\\"\"                                                                                         \n",
            "[31] \"\\\"American Airlines v. FareChase\\\"\"                                                                                                 \n",
            "[32] \"the original\"                                                                                                                       \n",
            "[33] \"\\\"American Airlines, FareChase Settle Suit\\\"\"                                                                                       \n",
            "[34] \"the original\"                                                                                                                       \n",
            "[35] \"Detecting and Blocking Site Scraping Attacks\"                                                                                       \n",
            "[36] \"\\\"Controversy Surrounds 'Screen Scrapers': Software Helps Users Access Web Sites But Activity by Competitors Comes Under Scrutiny\\\"\"\n",
            "[37] \"the original\"                                                                                                                       \n",
            "[38] \"\\\"QVC Inc. v. Resultly LLC, No. 14-06714 (E.D. Pa. filed Nov. 24, 2014)\\\"\"                                                          \n",
            "[39] \"the original\"                                                                                                                       \n",
            "[40] \"\\\"QVC Inc. v. Resultly LLC, No. 14-06714 (E.D. Pa. filed Nov. 24, 2014)\\\"\"                                                          \n",
            "[41] \"\\\"QVC Sues Shopping App for Web Scraping That Allegedly Triggered Site Outage\\\"\"                                                    \n",
            "[42] \"\\\"Did Iqbal/Twombly Raise the Bar for Browsewrap Claims?\\\"\"                                                                         \n",
            "[43] \"the original\"                                                                                                                       \n",
            "[44] \"\\\"Can Scraping Non-Infringing Content Become Copyright Infringement... Because Of How Scrapers Work? | Techdirt\\\"\"                  \n",
            "[45] \"\\\"Facebook v. Power Ventures\\\"\"                                                                                                     \n",
            "[46] \"\\\"UDSKRIFT AF SØ- & HANDELSRETTENS DOMBOG\\\"\"                                                                                        \n",
            "[47] \"the original\"                                                                                                                       \n",
            "[48] \"\\\"High Court of Ireland Decisions >> Ryanair Ltd -v- Billigfluege.de GMBH 2010 IEHC 47 (26 February 2010)\\\"\"                        \n",
            "[49] \"\\\"Intellectual Property: Website Terms of Use\\\"\"                                                                                    \n",
            "[50] \"the original\"                                                                                                                       \n",
            "[51] \"\\\"La réutilisation des données publiquement accessibles en ligne à des fins de démarchage commercial | CNIL\\\"\"                      \n",
            "[52] \"\\\"Can You Still Perform Web Scraping With The New CNIL Guidelines?\\\"\"                                                               \n",
            "[53] \"\\\"Spam Act 2003: An overview for business\\\"\"                                                                                        \n",
            "[54] \"the original\"                                                                                                                       \n",
            "[55] \"\\\"Spam Act 2003: A practical guide for business\\\"\"                                                                                  \n",
            "[56] \"Breaking Fraud & Bot Detection Solutions\"                                                                                           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCLUSION:** Identified the Text Analytics Libraries in Python and R\n",
        "Performed simple experiments with these libraries in Python and R"
      ],
      "metadata": {
        "id": "NdaUCtMaK-Ow"
      }
    }
  ]
}